{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we must import the needed modules to load and work with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.colors as colors\n",
    "import gen_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now simulate the data and package this information in such a way that it persumably could come to the Crowd-Sourcing Classifier (CC) from the Zooniverse server.\n",
    "\n",
    "We have four sets of variables to create and discuss as we go through this notebook.\n",
    "\n",
    "retired_images: A panda structure with columns \n",
    "\n",
    "    'imageID' (int) - ID of the image\n",
    "    'class' (int) - what class the image was retired as\n",
    "\n",
    "conf_matrices: A panda structure with columns \n",
    "\n",
    "    'userID' (int) - ID of user\n",
    "    'conf_matrix' (np.array) - the confusion matrix of the user\n",
    "\n",
    "PP_matrices: A panda structure with columns \n",
    "\n",
    "    'imageID' (int) - ID of the image\n",
    "    'pp_matrix' (np.array) - the prior (if it exists) of the image based on user evaluations\n",
    "\n",
    "images: A panda structure with columns\n",
    "\n",
    "    'type' (string) - A label, either 'T' or 'G', to determine if the image \n",
    "    is a ML classified label or a pre-labelled \"golden\" image\n",
    "\n",
    "    'labels' (np.array) - A 1XN row vector where N is the number\n",
    "    of labels this image has been given at a certain time. Each column\n",
    "    is a different answer that is associated with a different user.\n",
    "    This takes us to the next column\n",
    "\n",
    "    'userIDs' (np.array) - A 1XN row vector where N is the number of labels \n",
    "    this image has been given. Each userID is associated with one label\n",
    "    applied to the image in the labels column\n",
    "\n",
    "    'ML_posterior' (np.array) - A 1XC row vector where C is the number of \n",
    "    pre-determined morphologies that the classifier has been trained on.\n",
    "    Each column is the ML confidence that the image belongs in one of the C classes\n",
    "\n",
    "    'truelabel' (int) - For images labelled 'T' this values is set to -1\n",
    "    but for images labelled 'G' This value indicates the \"true\" class\n",
    "    that this image belongs in for the purposes of comparing a citizens\n",
    "    classification with this true label.\n",
    "       \n",
    "    'imageID' (int) - ID of the image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# retired images and PP_matrices starts out blank as we assume we have not seen any images at the outset of the CC. A\n",
    "retired_images = pd.DataFrame({ 'imageID' : [], 'class' : []})\n",
    "PP_matrices    = pd.DataFrame({ 'imageID' : [],'pp_matrix' : []})\n",
    "images,conf_matrices = gen_data.gen_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk about the conf_matrices variable that stores the confusion matrices of all the users. This information which is a panda structure will be a NX2 array where N is the number of users. In each row, we have one column that contains the CXC \"confusion matrix\", where C is the number of classes, for that user and another that contains the ID of that user. A perfectly skilled user would only have values on the diagonal of this matrix and all off diagonal values indicate wrong answers were given. Right or wrong answers are given with respect to either golden images which are pre-labelled images or to images that are retired through the pipeline. How we identify those images from the Zooniverse site that are in this \"golden set\" will come later. To illustrate this we'll print out one users simualted confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 189.    5.    2.    5.    1.    1.    5.    2.    5.    3.    4.    2.\n",
      "     4.    2.    1.]\n",
      " [   4.  182.    4.    1.    1.    2.    2.    2.    5.    3.    3.    3.\n",
      "     1.    5.    4.]\n",
      " [   4.    4.  193.    3.    1.    3.    4.    4.    2.    5.    4.    5.\n",
      "     5.    5.    1.]\n",
      " [   1.    3.    3.  195.    5.    1.    2.    3.    3.    5.    3.    5.\n",
      "     4.    4.    3.]\n",
      " [   2.    3.    1.    1.  190.    2.    5.    3.    3.    4.    2.    2.\n",
      "     4.    2.    1.]\n",
      " [   1.    3.    2.    2.    4.  206.    4.    5.    2.    1.    2.    5.\n",
      "     5.    4.    4.]\n",
      " [   3.    3.    1.    2.    5.    5.  182.    5.    4.    2.    4.    1.\n",
      "     5.    2.    3.]\n",
      " [   1.    4.    1.    5.    2.    5.    4.  218.    3.    3.    4.    2.\n",
      "     1.    2.    5.]\n",
      " [   4.    5.    3.    1.    1.    4.    4.    5.  188.    1.    5.    4.\n",
      "     4.    1.    3.]\n",
      " [   5.    2.    2.    3.    5.    1.    3.    5.    4.  214.    4.    4.\n",
      "     5.    2.    2.]\n",
      " [   2.    1.    3.    1.    1.    2.    5.    4.    1.    5.  197.    5.\n",
      "     4.    5.    4.]\n",
      " [   1.    3.    4.    5.    2.    1.    1.    5.    2.    1.    1.  218.\n",
      "     3.    3.    2.]\n",
      " [   4.    4.    1.    2.    4.    5.    5.    1.    1.    1.    4.    1.\n",
      "   189.    1.    3.]\n",
      " [   3.    5.    3.    3.    4.    5.    1.    1.    2.    3.    4.    4.\n",
      "     3.  219.    5.]\n",
      " [   3.    5.    2.    1.    2.    2.    1.    1.    1.    2.    3.    1.\n",
      "     5.    3.  211.]]\n"
     ]
    }
   ],
   "source": [
    "sample_conf_matrix = np.load('sample_conf_matrix.npy')\n",
    "print(sample_conf_matrix) #print sample confusion matrix of a user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the value in the first row, second column is a 5. This indicates that the user classified 5 images in class 2, when the image was \"retired\", or placed in, class 1. Confusion matrices are used to indicate a user's skill level.\n",
    "\n",
    "Here's a good visualization of the matrix in heat map form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.matshow(sample_conf_matrix, cmap='Blues', norm=colors.LogNorm(vmin=data.min(), vmax=data.max()))\n",
    "plt.colorbar()\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Classes')\n",
    "plt.title('Visualization of confusion matrix')\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(np.arange(0,15,1))\n",
    "ax.set_yticks(np.arange(0,15,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this heatmap changes everytime the notebook is run we can only make general comments. Ideally, the user would have a dark blue diagonal tiles and white tiles everywhere else. The darker the off tiles are the worse user is at classifying. That being said, sometimes we can expect that a given class, say class 6, will have a dark blue spot along the diagonal where it intersects with column 6 and is otherwise white except for one other column which would indicate the other class that this user confuses class 6 with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will define the main function that evaluates the batches of images labeled by users. The Zooniverse server (specifically Nero https://github.com/zooniverse/nero) will continuously send data (image batches) to be evaluated. \n",
    "\n",
    "Lets look at the data we would expect to have for every image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type                                                            T\n",
      "labels          [12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 1...\n",
      "userIDs         [5, 2, 9, 22, 1, 23, 24, 21, 7, 0, 14, 3, 20, ...\n",
      "ML_posterior    [0.0197605730029, 0.0197605730029, 0.019760573...\n",
      "truelabel                                                      -1\n",
      "imageID                                                       135\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "sample_image = pd.read_pickle('sample_image')\n",
    "print(sample_image) #print data for sample image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that's a lot of information for one image. Lets breakdown what each of these variables represents for a given image. In the next section, we will discuss various ways to access and utilize these panda structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imageID: 135 \n",
      "\n",
      "userIDs: [ 5  2  9 22  1 23 24 21  7  0 14  3 20 18 19  6 13 15 25 17 11 16 10  8 12] \n",
      "\n",
      "labels: [12 12 12 12 12 12 12 12 12 12 12 12  6 12  1  4 11  8 10  6  6  1  8  0 13]\n"
     ]
    }
   ],
   "source": [
    "print(\"imageID:\", sample_image['imageID'], \"\\n\")\n",
    "\n",
    "print(\"userIDs:\", sample_image['userIDs'], \"\\n\")\n",
    "\n",
    "print(\"labels:\", sample_image['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "userIDs: the ID of the users who classified this image\n",
    "\n",
    "imageID: the ID of the this image\n",
    "\n",
    "labels: the classification made by the users for this image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: T \n",
      "\n",
      "ML_posterior: [ 0.01976057  0.01976057  0.01976057  0.01976057  0.01976057  0.01976057\n",
      "  0.01976057  0.01976057  0.01976057  0.01976057  0.01976057  0.01976057\n",
      "  0.72335198  0.01976057  0.01976057] \n",
      "\n",
      "true_label: -1\n"
     ]
    }
   ],
   "source": [
    "print(\"type:\", sample_image['type'], \"\\n\")\n",
    "\n",
    "print(\"ML_posterior:\", sample_image['ML_posterior'], \"\\n\")\n",
    "\n",
    "print(\"true_label:\", sample_image['truelabel'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does all this information mean?\n",
    "\n",
    "This image has type 'T', meaning it is NOT a golden image and has only received labelling from the Machine Learning Classifier. If the type was 'G', then this image has been pre-labelled.\n",
    "\n",
    "From the ML_posterior matrix, we can see that the Machine Learning classifer is 79.95% sure this image is in the 14th class, and  1.43% sure it is in each other class.\n",
    "\n",
    "true_label mainly has meaning for images that have been pre-labelled. If it has been pre-labelled then it has a class associated with it already. If it has not been pre-labelled, we assign a label of -1.\n",
    "\n",
    "Now that we understand the structure of 'images' and how to access specific image information, lets move on to evaluating image and user classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (<ipython-input-26-89a0b26db994>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-26-89a0b26db994>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    print images.iloc[-1,:]['type'], \"\\n\"\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "print images.iloc[-1,:]['type'], \"\\n\"\n",
    "\n",
    "print images.iloc[-1,:]['truelabel'], \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize R_lim, the limit on how many people can look at an image, and it is not retired, before it is passed onto a higher skill level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R_lim = 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize N, the number of images in a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = images['type'].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize C, the number of morphologies (classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " for i in range(N):\n",
    "        if images['type'][i] == 'T':\n",
    "            C = images['ML_posterior'][i].size\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a flat prior. Essentially this means before any more information is known, we assume each image has an equal probability of being in each of the 15 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.06666667  0.06666667  0.06666667  0.06666667  0.06666667  0.06666667\n",
      "   0.06666667  0.06666667  0.06666667  0.06666667  0.06666667  0.06666667\n",
      "   0.06666667  0.06666667  0.06666667]]\n"
     ]
    }
   ],
   "source": [
    "priors = np.ones((1,C))/C\n",
    "print priors"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Initialize variable t. \n",
    "\n",
    "t is a CX1 column vector where C is the number of pre-determined morphologies and where each row is the predetermined certainty threshold that an image must surpass to be considered part of class C. Here all classes have the same threshold but in realty different categories will have more difficult or more relaxed thresholds for determination of class and, as a result, \n",
    "retirability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = .4*np.ones((C,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a matrix that holds the decision for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " dec_matrix = np.zeros((1,N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a class matrix that holds the True labels of each image. (corresponds one to one to decision matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_matrix = np.zeros((1,N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize a list to hold the pp_matrices for each images. We'll talk about the importance of pp_matrices later. Note that pp_matrix and posterior matrix may be used interchangeably. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pp_matrices_rack = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at how a decision for an image is made. The decider function takes in an images posterior matrix, Machine learning decision, number of annotators, and R_lim as arguments, and uses that information to decide the next step for the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decider(pp_matrix, ML_dec, t, R_lim, num_annotators): #define the decider function with given arguments\n",
    "    pp_matrix2 = np.hstack((pp_matrix, ML_dec.reshape((15,1)))) #Include ML_decision in posterior matrix\n",
    "    v = np.sum(pp_matrix2, axis=1)/np.sum(np.sum(pp_matrix)) #create vector of normalized sums of pp_matrix2 \n",
    "    maximum = np.amax(v) #initialize maximum, max value of v\n",
    "    maxIdx = np.argmax(v) #initialize maxIdx, index of max value of v\n",
    "\n",
    "    if maximum >= t[maxIdx]: #if maximum is above threshold for that specific class\n",
    "\n",
    "        decision = 1 #retire the image\n",
    "        print('Image is retired')\n",
    "\n",
    "    elif num_annotators >= R_lim: #if more than R_lim annotators have looked at image and no decision reached\n",
    "\n",
    "        decision = 2 #pass image on to next user skill class\n",
    "        print('Image is given to the upper class')\n",
    "\n",
    "    else: #if fewer than R_lim annotators have looked at image\n",
    "\n",
    "        decision = 3 #keep image in same class\n",
    "        print('More labels are needed for the image')\n",
    "\n",
    "    image_class = maxIdx #set image_class \n",
    "\n",
    "    return decision, image_class #return the decision, and image class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next chunk of code is used to update users confusion matrices, images posterior matrices (PP_matrix), and to make decisions on the future of an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is given to the upper class\n",
      "Image is given to the upper class\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is given to the upper class\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is given to the upper class\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is given to the upper class\n",
      "More labels are needed for the image\n",
      "Image is retired\n",
      "Image is given to the upper class\n",
      "Image is given to the upper class\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is given to the upper class\n",
      "Image is retired\n",
      "Image is given to the upper class\n",
      "Image is retired\n",
      "Image is given to the upper class\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is given to the upper class\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "More labels are needed for the image\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is given to the upper class\n",
      "Image is given to the upper class\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is given to the upper class\n",
      "Image is given to the upper class\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is given to the upper class\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is given to the upper class\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is given to the upper class\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is given to the upper class\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is given to the upper class\n",
      "More labels are needed for the image\n",
      "Image is given to the upper class\n",
      "More labels are needed for the image\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is retired\n",
      "Image is given to the upper class\n",
      "The image is from the training set\n",
      "The image is from the training set\n",
      "The image is from the training set\n",
      "The image is from the training set\n",
      "The image is from the training set\n",
      "The image is from the training set\n",
      "The image is from the training set\n",
      "The image is from the training set\n",
      "The image is from the training set\n",
      "The image is from the training set\n",
      "The image is from the training set\n",
      "The image is from the training set\n",
      "The image is from the training set\n",
      "The image is from the training set\n",
      "The image is from the training set\n",
      "The image is from the training set\n",
      "The image is from the training set\n",
      "The image is from the training set\n",
      "The image is from the training set\n",
      "The image is from the training set\n"
     ]
    }
   ],
   "source": [
    "    for i in range(N):\n",
    "\n",
    "        if images['type'][i] == 'G': #check if golden set image\n",
    "            labels  = images['labels'][i] #take citizen labels of image\n",
    "            userIDs = images['userIDs'][i] #take IDs of citizens who label image\n",
    "            tlabel  = images['truelabel'][i] #take true label of image\n",
    "\n",
    "            for ii in range(userIDs.size): #iterate over user IDs of image\n",
    "\n",
    "                indicator = 0\n",
    "\n",
    "                for cc in range(len(conf_matrices)): #iterate over confusion matrices\n",
    "\n",
    "                    if userIDs[ii] == conf_matrices['userID'][cc]: #if user is registered\n",
    "\n",
    "                        #take confusion matrix of citizen and index by true label, label given by user and update the confusion matrix at that entry.\n",
    "                        conf_matrices['conf_matrix'][cc][tlabel,labels[ii]] += 1\n",
    "                        indicator = 1\n",
    "\n",
    "                if indicator == 0: #if user not registered\n",
    "\n",
    "                    dummy_matrix = np.zeros((C,C)) #create dummy matrix\n",
    "                    dummy_matrix[tlabel,labels[ii]] += 1 #update dummy matrix\n",
    "                    tmp = pd.DataFrame({ 'userID' : [userIDs[ii]],'conf_matrix' : [dummy_matrix]},index = [len(conf_matrices)])\n",
    "                    conf_matrices = conf_matrices.append(tmp)\n",
    "\n",
    "            dec_matrix[0,i] = 0 #since it is a training image, no decision is made\n",
    "            class_matrix[0,i] = tlabel #class of image is its true label\n",
    "            pp_matrices_rack.append([0])\n",
    "            print('The image is from the training set')\n",
    "\n",
    "        else: #if image not in golden set, i.e. has ML label but no true label\n",
    "\n",
    "            indicator1 = 0\n",
    "\n",
    "            for kk in range(len(retired_images)): #loop over retired images\n",
    "\n",
    "                if images['imageID'][i] == retired_images['imageID'][kk]: #if image is retired\n",
    "                    indicator1 = 1\n",
    "                    dec_matrix[0,i] = -1 #give invalid decision\n",
    "                    pp_matrices_rack.append([0])\n",
    "                    break\n",
    "\n",
    "            if indicator1 == 0: #if image is not retired\n",
    "\n",
    "                labels           = images['labels'][i] #take citizen labels of image\n",
    "                userIDs          = images['userIDs'][i] #take IDs of citizens who label image\n",
    "                num_annotators   = labels.size #define number of citizens who annotate image\n",
    "                ML_dec           = images['ML_posterior'][i] #take ML posteriors of image\n",
    "                imageID          = images['imageID'][i] #take ID of image\n",
    "                image_prior      = priors #set priors for image to original priors\n",
    "\n",
    "                for y in range(len(PP_matrices)): #iterate over posterior matrices\n",
    "\n",
    "                    if imageID == PP_matrices['imageID'][y]: #find posterior matrix for the image\n",
    "                        image_prior = np.sum(PP_matrices['pp_matrix'][y],axis=1)/np.sum(PP_matrices['pp_matrix'][y]) #if image labeled but not retired, PP_matrix information is used in the place of priors\n",
    "                        break\n",
    "\n",
    "                pp_matrix = np.zeros((C,num_annotators)) #create posterior matrix\n",
    "                \n",
    "                for k in range(num_annotators): #iterate over citizens that labeled image\n",
    "                    for iN in range(len(conf_matrices)): #iterate over confusion matrices\n",
    "\n",
    "                        if userIDs[k] == conf_matrices['userID'][iN]: #find confusion matrix corresponding to citizen\n",
    "\n",
    "                            conf = conf_matrices['conf_matrix'][iN] #take confusion matrix of citizen\n",
    "                            break\n",
    "\n",
    "                    conf_divided,x,z,s = np.linalg.lstsq(np.diag(sum(conf,2)),conf) #calculate p(l|j) value\n",
    "\n",
    "                    for j in range(C): #iterate over classes\n",
    "\n",
    "                        pp_matrix[j,k] = (conf_divided[j,labels[k]]*priors[0][j])/sum(conf_divided[:,labels[k]]*priors[0]) #calculate posteriors\n",
    "                pp_matrices_rack.append(pp_matrix) #assign values to pp_matrices_rack\n",
    "\n",
    "\n",
    "                dec_matrix[0,i], class_matrix[0,i] = decider(pp_matrix, ML_dec, t, R_lim, num_annotators) #make decisions for each image in batch\n",
    "    for i in range(N):\n",
    "        if dec_matrix[0,i] == 1: #if image is retired\n",
    "            labels = images['labels'][i] #the citizen label of the image is taken\n",
    "            userIDs = images['userIDs'][i] #the IDs of the citizens that labeld that image are taken\n",
    "            for ii in range(userIDs.size): #iterate over user IDs of image\n",
    "\n",
    "                indicator2 = 0\n",
    "\n",
    "                for cc in range(len(conf_matrices)): #iterate over confusion matrices\n",
    "\n",
    "                    if userIDs[ii] == conf_matrices['userID'][cc]: #if user is registered\n",
    "\n",
    "                        #take confusion matrix of citizen and index by true label, label given by user and update the confusion matrix at that entry.\n",
    "                        conf_matrices['conf_matrix'][cc][tlabel,labels[ii]] += 1\n",
    "                        indicator2 = 1\n",
    "\n",
    "                if indicator2 == 0: #if user not registered\n",
    "\n",
    "                    dummy_matrix = np.zeros((C,C)) #create dummy matrix\n",
    "                    dummy_matrix[tlabel,labels[ii]] += 1 #update dummy matrix\n",
    "                    tmp = pd.DataFrame({ 'userID' : [userIDs[ii]],'conf_matrix' : [dummy_matrix]},index = [len(conf_matrices)])\n",
    "                    conf_matrices = conf_matrices.append(tmp)\n",
    "\n",
    "    # Ordering the images and sending/saving them\n",
    "    counter1 = len(retired_images)\n",
    "    counter2 = len(PP_matrices)\n",
    "\n",
    "    for i in range(N):\n",
    "\n",
    "        if dec_matrix[0,i] == 1: # if it is decided to be retired\n",
    "            tmp = pd.DataFrame({ 'imageID' : [images['imageID'][i]],'class' : [class_matrix[0,i]]},index = [counter1])\n",
    "            retired_images = retired_images.append(tmp)\n",
    "\n",
    "            counter1 = counter1 + 1\n",
    "\n",
    "        elif dec_matrix[0,i] == 2 or dec_matrix[0,i] == 3:  #if the decision is forwarding to the upper class or wait for more labels\n",
    "\n",
    "            dummy_decider = 1\n",
    "\n",
    "            for y in range(len(PP_matrices)):        #in case the image was waiting for more labels beforehand\n",
    "\n",
    "                if images['imageID'][i] == PP_matrices['imageID'][y]:\n",
    "                    PP_matrices['pp_matrix'][y] = pp_matrices_rack[i]      #the PP matrix is overwritten.\n",
    "                    dummy_decider = 0\n",
    "                    break\n",
    "\n",
    "            if dummy_decider:\n",
    "                tmp = pd.DataFrame({ 'imageID' : [images['imageID'][i]],'pp_matrix' : [pp_matrices_rack[i]]},index = [counter2])\n",
    "                PP_matrices = PP_matrices.append(tmp)\n",
    "\n",
    "                counter2 = counter2 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now lets take a look at how the decider function works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image is given to the upper class\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pp_matrix2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-7d7165d935b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdecider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpp_matrix_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mML_dec_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_lim_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_annotators_sample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#call decider function with sample arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mpp_matrix2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pp_matrix2' is not defined"
     ]
    }
   ],
   "source": [
    "#Sample arguments for decider function \n",
    "pp_matrix_sample = PP_matrices['pp_matrix'][0]\n",
    "ML_dec_sample = np.array([.1, .05, .05, .05, .05, .6, .001, .014, .1, .002, .005, .0002, 0, 0, 0])\n",
    "R_lim_sample = 20\n",
    "no_annotators_sample = len(PP_matrices['pp_matrix'][0][0])\n",
    "\n",
    "\n",
    "\n",
    "decider(pp_matrix_sample, ML_dec_sample, t, R_lim_sample, no_annotators_sample) #call decider function with sample arguments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
