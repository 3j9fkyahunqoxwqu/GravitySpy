#!/usr/bin/env python

# ---- Import standard modules to the python path.

from __future__ import division

import sys
import os
import random
import string
import shutil
import argparse
import json
import rlcompleter
import operator
from configparser import ConfigParser

from panoptes_client import *

import pandas as pd
import numpy as np
import h5py

from scipy import signal
from scipy.interpolate import InterpolatedUnivariateSpline

from sqlalchemy.engine import create_engine

from gwpy.timeseries import TimeSeries
from gwpy.spectrogram import Spectrogram
from gwpy.segments import Segment

from gravityspy import __version__
from gravityspy.plot.plot import plot_qtransform
from gravityspy.api.project import GravitySpyProject
import gravityspy.ml.read_image as read_image
import gravityspy.ml.labelling_test_glitches as label_glitches
from gravityspy.utils import log

###############################################################################
##########################                             ########################
##########################   Func: parse_commandline   ########################
##########################                             ########################
###############################################################################
# Definite Command line arguments here

def parse_commandline():
    """Parse the arguments given on the command-line.
    """
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument('-V', '--version', action='version',
                        version=__version__)
    parser.add_argument("--inifile", help="Name of ini file of params",
                        required=True)
    parser.add_argument("--eventTime", type=float,
                        help="Trigger time of the glitch", required=True)
    parser.add_argument("--uniqueID", action="store_true", default=False,
                        help="Is this image being generated for "
                        "the GravitySpy project, if so you must assign a "
                        "uniqueID string to label the images instead of "
                        "GPS time")
    parser.add_argument("--ID",
                        help="The uniqueID string to be supplied with "
                             "--uniqueID")
    parser.add_argument("--outDir", help="Outdir for images")
    parser.add_argument("--path-to-cnn-model",
                        help="Path to name of cnn model",
                        required=True)
    parser.add_argument("--path-to-semantic-file",
                        help="Path to name of similarity model",
                        required=True)
    parser.add_argument("--project-info-pickle",
                        help="This pickle file holds information"
                        "about what workflows a image with what"
                        "confidence label should go into",
                        required=True)
    parser.add_argument("--runML", action="store_true", default=False,
                        help="Run the ML classifer on the omega scans")
    parser.add_argument("--verbose", action="store_true", default=False,
                        help="Run in Verbose Mode")
    parser.add_argument("--HDF5", action="store_true", default=False,
                        help="Store triggers in local HDF5 table format")
    parser.add_argument("--PostgreSQL", action="store_true", default=False,
                        help="Store triggers in a remote PostgreSQL DB")
    args = parser.parse_args()


    return args

###############################################################################
##########################                     ################################
##########################      MAIN CODE      ################################
##########################                     ################################
###############################################################################

def main(inifile, eventTime, project_info_pickle, ID, outDir,
         path_to_cnn='/home/machinelearning/GravitySpy/gravityspy/ML/trained_model/multi_view_classifier.h5',
         path_to_similarity_search='/home/machinelearning/GravitySpy/gravityspy/ML/trained_model/semantic_idx_model.h5',
         uniqueID=True, runML=False, HDF5=False, PostgreSQL=False,
         verbose=False):

    if (runML == True) and (HDF5==False) and (PostgreSQL==False):
        raise ValueError('If you wish to run ML must select file format to '
                         'save with. Most cases HDF5=True is what you want')

    if not os.path.isfile(path_to_cnn):
        raise ValueError('The provided CNN model does not '
                         'exist.')
    if not os.path.isfile(path_to_similarity_search):
        raise ValueError('The provided similarity model does not '
                         'exist.')

    logger = log.Logger('Gravity Spy: OmegaScan')

    ###########################################################################
    #                                   Parse Ini File                        #
    ###########################################################################

    # ---- Create configuration-file-parser object and read parameters file.
    cp = ConfigParser()
    cp.read(inifile)
    logger.info('You have chosen the following ini file: {0}'.format(inifile))

    # ---- Read needed variables from [parameters] and [channels] sections.
    alwaysPlotFlag = cp.getint('parameters', 'alwaysPlotFlag')
    sampleFrequency = cp.getint('parameters', 'sampleFrequency')
    blockTime = cp.getint('parameters', 'blockTime')
    searchFrequencyRange = json.loads(cp.get('parameters',
                                             'searchFrequencyRange'))
    searchQRange = json.loads(cp.get('parameters', 'searchQRange'))
    searchMaximumEnergyLoss = cp.getfloat('parameters',
                                          'searchMaximumEnergyLoss')
    searchWindowDuration = cp.getfloat('parameters', 'searchWindowDuration')
    whiteNoiseFalseRate = cp.getfloat('parameters', 'whiteNoiseFalseRate')
    plotTimeRanges = json.loads(cp.get('parameters', 'plotTimeRanges'))
    plotFrequencyRange = json.loads(cp.get('parameters', 'plotFrequencyRange'))
    plotNormalizedERange = json.loads(cp.get('parameters',
                                             'plotNormalizedERange'))
    frameCacheFile = cp.get('channels', 'frameCacheFile')
    frameType = cp.get('channels', 'frameType')
    channelName = cp.get('channels', 'channelName')
    detectorName = channelName.split(':')[0]
    det = detectorName.split('1')[0]

    logger.info('You have chosen the following Q range: '
                '{0}'.format(searchQRange))
    logger.info('You have chosen the following search range: '
                '{0}'.format(searchFrequencyRange))

    ########################################################################
    #     Determine if this is a normal omega scan or a Gravityspy         #
    #    omega scan with unique ID. If Gravity spy then additional         #
    #    files and what not must be generated                              #
    ########################################################################

    if uniqueID:
        IDstring = ID
    else:
        IDstring = "{0:.2f}".format(eventTime)

    ###########################################################################
    #                           create output directory                       #
    ###########################################################################

    # if outputDirectory not specified, make one based on center time
    if outDir is None:
        outDirtmp = './scans'
    else:
        outDirtmp = os.path.join(outDir, IDstring, IDstring)
    outDirtmp += '/'

    # report status
    if not os.path.isdir(outDirtmp):
        if verbose:
            logger.info('creating event directory')
        os.makedirs(outDirtmp)
    if verbose:
        logger.info('outputDirectory:  {0}'.format(outDirtmp))

    ###########################################################################
    #               Process Channel Data                                      #
    ###########################################################################

    # find closest sample time to event time
    centerTime = np.floor(eventTime) + \
               np.round((eventTime - np.floor(eventTime)) * \
                     sampleFrequency) / sampleFrequency

    # determine segment start and stop times
    startTime = round(centerTime - blockTime / 2)
    stopTime = startTime + blockTime

    # Read in the data
    logger.info('Fetching Data...')
    data = TimeSeries.get(channelName, startTime, stopTime).astype('float64')

    # resample data
    logger.info('Resampling Data...')
    if data.sample_rate.decompose().value != sampleFrequency:
        data = data.resample(sampleFrequency)

    # Cropping the results before interpolation to save on time and memory
    # perform the q-transform
    specsgrams = []
    for iTimeWindow in plotTimeRanges:
        logger.info('Processing q_scan of duration: {0}'.format(iTimeWindow))
        durForPlot = iTimeWindow/2
        try:
            outseg = Segment(centerTime - durForPlot, centerTime + durForPlot)
            qScan = data.q_transform(qrange=tuple(searchQRange),
                                 frange=tuple(searchFrequencyRange),
                                 gps=centerTime, search=0.5, tres=0.002,
                                 fres=0.5, outseg=outseg, whiten=True)
            qValue = qScan.q
            qScan = qScan.crop(centerTime-iTimeWindow/2,
                               centerTime+iTimeWindow/2)
        except:
            outseg = Segment(centerTime - 2*durForPlot,
                             centerTime + 2*durForPlot)
            qScan = data.q_transform(qrange=tuple(searchQRange),
                                 frange=tuple(searchFrequencyRange),
                                 gps=centerTime, search=0.5, tres=0.002,
                                 fres=0.5, outseg=outseg, whiten=True)
            qValue = qScan.q
            qScan = qScan.crop(centerTime-iTimeWindow/2,
                               centerTime+iTimeWindow/2)
        specsgrams.append(qScan)

    logger.info('The most significant q value is {0}'.format(qValue))

    # Plot q_scans
    logger.info('Plotting q scans...')
    indFigAll, superFig = plot_qtransform(specsgrams,
                          plotNormalizedERange, plotTimeRanges,
                          detectorName, startTime)

    for idx, indFig in enumerate(indFigAll):
        dur = float(plotTimeRanges[idx])
        indFig.save(os.path.join(
                                 outDirtmp,
                                 detectorName + '_' + IDstring
                                 + '_spectrogram_' + str(dur) +'.png'
                                )
                   )

    superFig.save(os.path.join(outDirtmp, IDstring + '.png'),
                  bbox_inches='tight')

    if runML:
        # load the api gravityspy project cached class
        gspyproject = GravitySpyProject.load_project_from_cache(
                                            project_info_pickle)
        # Since we created the images in a
        # special temporary directory we can run os.listdir to get there full
        # names so we can convert the images into ML readable format.
        list_of_images = [ifile for ifile in os.listdir(outDirtmp) 
                          if 'spectrogram' in ifile]

        logger.info('Converting image to ML readable...')
        image_data_for_cnn = pd.DataFrame()
        image_data_for_si = pd.DataFrame()
        for idx, image in enumerate(list_of_images):
            logger.info('Converting {0}'.format(image))
            image_data_r, image_data_g, image_data_b = read_image.read_rgb(os.path.join(outDirtmp, image),
                                          resolution=0.3)
            image_data = read_image.read_grayscale(os.path.join(outDirtmp, image),
                                                   resolution=0.3)
            image_data_for_si[image] = [[image_data_r, image_data_g, image_data_b]]
            image_data_for_cnn[image] = [image_data]

        image_data_for_cnn['uniqueID'] = IDstring
        image_data_for_si['uniqueID'] = IDstring

        # Now label the image
        logger.info('Labelling image...')
        scores, MLlabel = label_glitches.label_glitches(image_data=image_data_for_cnn,
                                              model_name='{0}'.format(path_to_cnn),
                                              image_size=[140, 170],
                                              verbose=verbose)

        # extract confidence
        confidence = float(scores[0][MLlabel])

        # We must determine the columns that will be saved for this image.
        # First and foremost we want to determine the possible classes the image could be
        # get all the info about the workflows
        workflowDictSubjectSets = gspyproject.get_level_structure(IDfilter='O2')

        # Must determine classes from dict
        classes = sorted(workflowDictSubjectSets['2117'].keys())

        # Add on columns that are Gravity Spy specific
        classes.extend(["uniqueID", "Label", "workflow", "subjectset",
                        "Filename1", "Filename2", "Filename3", "Filename4",
                        "UploadFlag", "qvalue"])

        # Determine label
        Label = classes[MLlabel]

        logger.info('This image has received the following label: '
                    '{0} with {1} percent confidence'.format(Label, confidence))


        logger.info('Extracting Features'.format(image))
        features = label_glitches.get_multiview_feature_space(
                                                        image_data_for_si,
                                                        '{0}'.format(
                                                              path_to_similarity_search),
                                                        [140, 170],
                                                        False)


        # Determine features
        #logger.info('Extracting Features'.format(image))
        #features = label_glitches.get_feature_space(image_data=image_dataDF,
        #                                      semantic_model_adr='{0}'.format(pathToModel),
        #                                      image_size=[140, 170],
        #                                      verbose=verbose)

        features = pd.DataFrame(features)
        features['uniqueID'] = IDstring

        # Create directory called "Classified" were images that were successfully classified go.
        workFlow = 'Classified'
        finalPath = outDir + '/' + workFlow

        if not os.path.isdir(finalPath):
            os.makedirs(finalPath)

        # determine confidence values from ML
        scores = scores[0].tolist()
        # Append uniqueID to list so when we update sql we will know which entry to update
        scores.append(IDstring)
        # Append label
        scores.append(Label)

        # Determine subject set and workflow this should go to.
        for iWorkflow in workflowDictSubjectSets.keys():
            if Label in workflowDictSubjectSets[iWorkflow].keys():
                 if workflowDictSubjectSets[iWorkflow][Label][2][1] <= \
                        confidence <= \
                             workflowDictSubjectSets[iWorkflow][Label][2][0]:
                     workflowNum = int(workflowDictSubjectSets[iWorkflow][Label][0])
                     subjectSetNum = workflowDictSubjectSets[iWorkflow][Label][1]
                     break

        subject1 = '{0}/{1}_{2}_spectrogram_0.5.png'.format(finalPath, detectorName, IDstring)
        subject2 = '{0}/{1}_{2}_spectrogram_1.0.png'.format(finalPath, detectorName, IDstring)
        subject3 = '{0}/{1}_{2}_spectrogram_2.0.png'.format(finalPath, detectorName, IDstring)
        subject4 = '{0}/{1}_{2}_spectrogram_4.0.png'.format(finalPath, detectorName, IDstring)

        scores.append(workflowNum)
        scores.append(subjectSetNum)
        scores.append(subject1)
        scores.append(subject2)
        scores.append(subject3)
        scores.append(subject4)
        # Upload flag (defaults to 0 because image gets uploaded later)
        scores.append(0)
        scores.append(qValue)

        scoresTable = pd.DataFrame([scores],columns=classes)

        if PostgreSQL:
            engine = create_engine(
                                   'postgresql://{0}:{1}'\
                                   .format(os.environ['GRAVITYSPY_DATABASE_USER'],os.environ['GRAVITYSPY_DATABASE_PASSWD'])\
                                   + '@gravityspy.ciera.northwestern.edu:5432/gravityspy')
            columnDict = scoresTable.to_dict(orient='records')[0]
            SQLCommand = 'UPDATE glitches SET '
            for Column in columnDict:
                if isinstance(columnDict[Column], str):
                    SQLCommand = SQLCommand + '''\"{0}\" = \'{1}\', '''.format(Column,columnDict[Column])
                else:
                    SQLCommand = SQLCommand + '''\"{0}\" = {1}, '''.format(Column,columnDict[Column])
            SQLCommand = SQLCommand[:-2] + ' WHERE \"uniqueID\" = \'' + scoresTable.uniqueID.iloc[0] + "'"
            engine.execute(SQLCommand)
            features.to_sql('similarityindex', engine, index=False, if_exists='append')
        elif HDF5:
            scoresTable.to_hdf('{0}/ML_GSpy_{1}.h5'.format(outDir, IDstring), table=True, key='gspy_ML_classification')
            features.to_hdf('{0}/ML_GSpy_{1}.h5'.format(outDir, IDstring), table=True, key='featurespace')

        system_call = "mv {0}*.png {1}".format(outDirtmp, finalPath)
        os.system(system_call)
        shutil.rmtree(os.path.join(outDir, IDstring))

if __name__ == '__main__':
    args = parse_commandline()
    main(args.inifile, args.eventTime, args.project_info_pickle,
         args.ID, args.outDir, args.path_to_cnn_model, args.path_to_semantic_file,
         args.uniqueID, args.runML, args.HDF5, args.PostgreSQL, args.verbose)
